# -*- coding: utf-8 -*-
"""Network_Intrusion_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LEz5wY5iw6E3lv_snsfbUh4kzmyGpAOH

**NETWORK INTRUSION DETECTION SYSTEM USING MACHINE LEARNING ALGORITHMS**

**SPRINT 1: DATA EXPLORIZATION**

MOUNT GOOGLE DRIVE TO GOOGLE COLAB
"""

'''from google.colab import drive
drive.mount('/content/drive')'''

"""IMPORTING ALL NECESSARY LIBRARIES"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import LabelEncoder

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import time

"""LOADING THE UNSWNB-15 DATASET FOR NETWORK INTRUSION DETECTION"""

df=pd.read_csv("C:/Users/anjuk/OneDrive/Desktop/nids/UNSW_NB15 (2).csv")

df.shape

"""FEATURES IN DATASET INFORMATION"""

df.info()

"""CHECKING FOR MISSING VALUES"""

# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

"""CHECKING FOR DUPLICATE ROWS"""

# Check for duplicate rows
duplicate_rows = df.duplicated().sum()
print("Duplicate Rows:", duplicate_rows)

"""FEATURE NAMES LISTED"""

feature_names = df.columns
print("Feature Names:")
for feature in feature_names:
    print(feature)

"""DESCRIBE THE STATISTICS OF DATASET"""

df_numeric = df.select_dtypes(include=[np.number])
df_numeric.describe(include='all')

"""ATTACK PROPORTION IN THE DATASET"""

# Get the attack category counts
attack_cat_counts = df['attack_cat'].value_counts()

# Create a pie chart of the attack category counts
plt.pie(attack_cat_counts.values, labels=attack_cat_counts.index, autopct="%1.1f%%")
plt.title("Pie Chart of Attack Category in UNSW-NB15 Dataset")
plt.show()
# Get the attack category counts
attack_cat_counts = df['label'].value_counts()
print("\n")
# Create a pie chart of the attack category counts
plt.pie(attack_cat_counts.values, labels=attack_cat_counts.index, autopct="%1.1f%%")
plt.title("Pie Chart of Attack Category in UNSW-NB15 Dataset")
plt.show()

"""HISTOGRAM OF THE ATTACK PROPORTION"""

plt.hist(df['attack_cat'], color="cyan", edgecolor="blue")
plt.xlabel("attack_cat")
plt.ylabel("frequency")
plt.show()

"""COUNT PLOT OF CATEGORICAL FEATURES"""

# Select the four object data type columns
object_columns = ['proto', 'service', 'state', 'attack_cat']

# Create subplots for count plots
plt.figure(figsize=(16, 8))
for i, column in enumerate(object_columns, 1):
    plt.subplot(2, 2, i)
    sns.countplot(data=df, x=column)
    plt.title(f"Count Plot of {column}")
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""BOXPLOT OF NUMERIC FEATURES"""

feature_names=[ 'dur', 'spkts', 'dpkts', 'sbytes','dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss','sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin','tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth','response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm','ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm','is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm','ct_srv_dst', 'is_sm_ips_ports', 'label']

# Extract the  features from your dataset
_data = df[feature_names]

# Create a box plot for each of the features
plt.figure(figsize=(14, 6))  # Adjust the figure size as needed
sns.boxplot(data=_data)
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.title("Box Plots of  Features")
plt.xlabel("Features")
plt.ylabel("Values")
plt.show()

df_cat = df.select_dtypes(exclude=[np.number])
df_cat.describe(include='all')

df['attack_cat'].unique()

list_drop = ['id']

df.drop(list_drop,axis=1,inplace=True)

DEBUG =0

for feature in df_numeric.columns:
    if DEBUG == 1:
        print(feature)
        print('max = '+str(df_numeric[feature].max()))
        print('75th = '+str(df_numeric[feature].quantile(0.95)))
        print('median = '+str(df_numeric[feature].median()))
        print(df_numeric[feature].max()>10*df_numeric[feature].median())
        print('----------------------------------------------------')
    if df_numeric[feature].max()>10*df_numeric[feature].median() and df_numeric[feature].max()>10 :
        df[feature] = np.where(df[feature]<df[feature].quantile(0.95), df[feature], df[feature].quantile(0.95))

df_numeric = df.select_dtypes(include=[np.number])
df_numeric.describe(include='all')

df_numeric = df.select_dtypes(include=[np.number])
df_before = df_numeric.copy()
DEBUG = 0
for feature in df_numeric.columns:
    if DEBUG == 1:
        print(feature)
        print('nunique = '+str(df_numeric[feature].nunique()))
        print(df_numeric[feature].nunique()>50)
        print('----------------------------------------------------')
    if df_numeric[feature].nunique()>50:
        if df_numeric[feature].min()==0:
            df[feature] = np.log(df[feature]+1)
        else:
            df[feature] = np.log(df[feature])

df_numeric = df.select_dtypes(include=[np.number])

df_cat = df.select_dtypes(exclude=[np.number])
df_cat.describe(include='all')

DEBUG = 0
for feature in df_cat.columns:
    if DEBUG == 1:
        print(feature)
        print('nunique = '+str(df_cat[feature].nunique()))
        print(df_cat[feature].nunique()>6)
        print(sum(df[feature].isin(df[feature].value_counts().head().index)))
        print('----------------------------------------------------')

    if df_cat[feature].nunique()>6:
        df[feature] = np.where(df[feature].isin(df[feature].value_counts().head().index), df[feature], '-')

df_cat = df.select_dtypes(exclude=[np.number])
df_cat.describe(include='all')

proto_encoder = LabelEncoder()
service_encoder = LabelEncoder()
state_encoder = LabelEncoder()
attack_cat_encoder = LabelEncoder()

df['proto_encoded'] = proto_encoder.fit_transform(df['proto'])
df['service_encoded'] = service_encoder.fit_transform(df['service'])
df['state_encoded'] = state_encoder.fit_transform(df['state'])
df['attack_cat_encoded'] = attack_cat_encoder.fit_transform(df['attack_cat'])

df['proto'] = df['proto_encoded']
df['service'] = df['service_encoded']
df['state'] = df['state_encoded']
df['attack_cat'] = df['attack_cat_encoded']

if not df['proto'].dtype.name == 'category':
    del df['proto_encoded']

if not df['service'].dtype.name == 'category':
    del df['service_encoded']

if not df['state'].dtype.name == 'category':
    del df['state_encoded']

if not df['attack_cat'].dtype.name == 'category':
    del df['attack_cat_encoded']

import pandas as pd
import plotly.graph_objects as go
from sklearn.feature_selection import SelectKBest, chi2



# SelectKBest with chi2 score function
best_features = SelectKBest(score_func=chi2, k='all')

X = df.select_dtypes(include=['number'])  # Adjust the column selection as needed
y = df.iloc[:, -1]
fit = best_features.fit(X, y)

df_scores = pd.DataFrame(fit.scores_)
df_col = pd.DataFrame(X.columns)

feature_score = pd.concat([df_col, df_scores], axis=1)
feature_score.columns = ['feature', 'score']
feature_score.sort_values(by=['score'], ascending=False, inplace=True)  # Use 'ascending=False' to sort in descending order

# Create a bar plot for the top 20 features
top_20_features = feature_score.head(20)

fig = go.Figure(go.Bar(
    x=top_20_features['score'],
    y=top_20_features['feature'],
    orientation='h'
))

fig.update_layout(title="Top 20 Features",
                  height=800,  # Adjust the height as needed
                  xaxis_title="Score",
                  yaxis_title="Feature",
                  showlegend=False,
                  )

fig.show()

df['attack_cat'].unique()

# Specify the top 20 feature names
feature_names = ['sttl', 'swin', 'dwin', 'dpkts', 'spkts', 'dload', 'ct_dst_src_ltm', 'dloss', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'stcpb', 'dtcpb', 'ct_srv_dst', 'ct_srv_src', 'dbytes', 'ct_dst_ltm', 'ct_src_ltm', 'sloss', 'dmean', 'dttl']

# Extract the top 20 features from your dataset
top_20_data = df[feature_names]

# Create a box plot for each of the top 20 features
plt.figure(figsize=(14, 6))  # Adjust the figure size as needed
sns.boxplot(data=top_20_data)
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.title("Box Plots of Top 20 Features")
plt.xlabel("Features")
plt.ylabel("Values")
plt.show()

Y=df['label']
Y.shape

X= df[top_20_features['feature']]
X.columns

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a StandardScaler instance
scaler = StandardScaler()

# Fit and transform the scaler on the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the testing data using the same scaler
X_test_scaled = scaler.transform(X_test)

# Now, X_train_scaled and X_test_scaled contain the standardized features

import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
import pandas as pd
import numpy as np
from time import time

# Initialize empty lists to store performance metrics
accuracies = []
losses = []
epoch_performance = []

# Specify the number of "epochs" (iterations)
n_epochs = 10

# Create a DataFrame to store the performance metrics
model_performance = pd.DataFrame(columns=[
    'Epoch', 'Accuracy', 'Recall', 'Precision', 'F1-Score',
    'Time to Train', 'Time to Predict', 'Total Time'
])



# Train the Decision Tree classifier

# Train the model for 20 epochs and collect performance metrics
for epoch in range(n_epochs):
    start = time()
    decision_tree_model = DecisionTreeClassifier(random_state=42)
  # Split the training data into batches
    X_train_batches = np.array_split(X_train_scaled, 10)
    y_train_batches = np.array_split(y_train, 10)

# Train the decision tree on each batch
    for X_train_batch, y_train_batch in zip(X_train_batches, y_train_batches):

      decision_tree_model.fit(X_train_batch, y_train_batch)
      end_train = time()
      y_pred = decision_tree_model.predict(X_test_scaled)
      end_predict = time()
      accuracy = accuracy_score(y_test, y_pred)
      loss = 1 - accuracy
      recall_dt = recall_score(y_test, y_pred, average='weighted')
      precision_dt = precision_score(y_test, y_pred, average='weighted')
      f1s_dt = f1_score(y_test, y_pred, average='weighted')
      training_time = end_train - start
      prediction_time = end_predict - end_train
      total_time = end_predict - start

     # Collect performance metrics for each epoch

    # Append accuracy and loss to the lists
      accuracies.append(accuracy)
      losses.append(loss)

    # Append performance metrics to the DataFrame
      model_performance.loc[epoch] = [
        epoch + 1, accuracy, recall_dt, precision_dt, f1s_dt, training_time, prediction_time, total_time
      ]

from rich.console import Console
from rich.table import Table

# Create a rich table using the `rich` module
table = Table()
table.add_column("Epoch", style="bold", width=10)
table.add_column("Accuracy", width=15)
table.add_column("Recall", width=15)
table.add_column("Precision", width=15)
table.add_column("F1-Score", width=15)
table.add_column("Time to Train", width=15)
table.add_column("Time to Predict", width=15)
table.add_column("Total Time", width=15)
console = Console()

# Add rows to the table for each epoch's performance metrics
for row in model_performance.itertuples():
    # Convert float values to strings
    row_values = [str(value) for value in row[1:]]
    table.add_row(*row_values)

# Print the rich table using the render() function
console.print(table)

from rich import console
from rich.table import Table

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# Initialize a list to store accuracy values
accuracies = []
losses = []
epoch_performance = []

# Specify the number of "epochs" (iterations)
n_epochs = 10
model_performance = pd.DataFrame(index=np.arange(1, n_epochs + 1), columns=['Epoch', 'Accuracy', 'Recall', 'Precision', 'F1-Score', 'Time to Train', 'Time to Predict', 'Total Time'])

# Create a Random Forest classifier
random_forest_model = RandomForestClassifier(n_estimators=10, random_state=42)

# Perform the specified number of "epochs" (iterations)
for epoch in range(n_epochs):
    # Train the model on your training data (X_train, y_train)
    start = time()

    random_forest_model.fit(X_train_scaled, y_train)
    end_train = time()

    # Make predictions on the validation data (X_val, y_val)
    y_pred = random_forest_model.predict(X_test_scaled)
    end_predict = time()

    # Calculate accuracy and other performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    recall_rf = recall_score(y_test, y_pred, average='weighted')
    precision_rf = precision_score(y_test, y_pred, average='weighted')
    f1s_rf = f1_score(y_test, y_pred, average='weighted')
    loss = 1 - accuracy

    # Collect performance metrics for each epoch
    epoch_performance.append([epoch + 1, accuracy, recall_rf, precision_rf, f1s_rf, end_train - start, end_predict - end_train, end_predict - start])

    # Append accuracy and loss to the lists
    accuracies.append(accuracy)
    losses.append(loss)

    # Fill the performance matrix with current epoch's metrics
    model_performance.loc[epoch] = [epoch + 1, accuracy, recall_rf, precision_rf, f1s_rf, end_train - start, end_predict - end_train, end_predict - start]

# Create a rich table using the `rich` module
table = Table()
table.add_column("Epoch", style="bold", width=10)
table.add_column("Accuracy", width=15)
table.add_column("Recall", width=15)
table.add_column("Precision", width=15)
table.add_column("F1-Score", width=15)
table.add_column("Time to Train", width=15)
table.add_column("Time to Predict", width=15)
table.add_column("Total Time", width=15)
for row in model_performance.itertuples():
    # Convert float values to strings
    row_values = [str(value) for value in row[1:]]
    table.add_row(*row_values)

from rich.console import Console

# Create a console object
console = Console()

# Print the rich table using the render() function
console.print(table)

import pickle

# Save the trained model to a pickle file
with open('random_forest_model.pkl', 'wb') as f:
    pickle.dump(random_forest_model, f)

with open('random_forest_model.pkl', 'rb') as f:
  # Load the pickled object
  loaded_object = pickle.load(f)
  print(loaded_object)

import os

# Verify the pickle file
if os.stat('random_forest_model.pkl').st_size > 0:
    print("Model saved successfully to 'my_model.pkl'")
else:
    print("Failed to save model to 'my_model.pkl'")

# Save the trained model to a pickle file
with open('decision_tree_model.pkl', 'wb') as f:
    pickle.dump(decision_tree_model, f)

with open('decision_tree_model.pkl', 'rb') as f:
  # Load the pickled object
  loaded_object = pickle.load(f)
  print(loaded_object)

import os

# Verify the pickle file
if os.stat('decision_tree_model.pkl').st_size > 0:
    print("Model saved successfully to 'my_model.pkl'")
else:
    print("Failed to save model to 'my_model.pkl'")